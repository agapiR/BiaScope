{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import utilities\n",
    "from utilities import group_unfairness_score, group_unfairness_scores\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified group fairness score precomputation to include larger values of k\n",
    "# id, pos_x, pos_y, inFoRM, proj_x, proj_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# networkx positions \n",
    "def get_node_pos(G, node_features):\n",
    "    G = nx.read_edgelist(edgelist_file)\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    start = time.time()\n",
    "    nodePos = nx.spring_layout(G, seed=42)\n",
    "    end = time.time()\n",
    "\n",
    "    for node in nodes:\n",
    "        if node not in node_features:\n",
    "            node_features[node] = {\"id\": node}\n",
    "        node_features[node][\"pos_x\"] = nodePos[node][0]\n",
    "        node_features[node][\"pos_y\"] = nodePos[node][1]\n",
    "    print(\"Spring Layout Elapsed Time: {}\".format(int(end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_proj(G, embeddings, node_features):\n",
    "    start = time.time()\n",
    "    embeddings_pca = PCA(n_components=2).fit_transform(embeddings)\n",
    "    end = time.time()\n",
    "    nodes = list(G.nodes())\n",
    "    for i in range(len(nodes)):\n",
    "        node_features[nodes[i]][\"proj_x\"] = embeddings_pca[i][0]\n",
    "        node_features[nodes[i]][\"proj_y\"] = embeddings_pca[i][1]\n",
    "\n",
    "    print(\"PCA Elapsed Time: {}\".format(int(end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inFoRM(G, embeddings, node_features):\n",
    "    adj_matrix = nx.to_numpy_array(G)\n",
    "    assert(adj_matrix.max() == 1)\n",
    "    start = time.time()\n",
    "    inFoRM_scores = utilities.unfairness_scores_normalized(embeddings, adj_matrix, G)\n",
    "    end = time.time()\n",
    "    nodes = list(G.nodes())\n",
    "    for i in range(len(inFoRM_scores)):\n",
    "        node_features[nodes[i]][\"InFoRM\"] = inFoRM_scores[i]\n",
    "    print(\"InFoRM Elapsed Time: {}\".format(int(end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_metadata = {\"Facebook\": {\"edgelist\": \"../edgelists/facebook_combined.edgelist\"},\n",
    "#                  \"LastFM\": {\"edgelist\": \"../edgelists/lastfm_asia_edges.edgelist\"},\n",
    "#                  \"wikipedia\": {\"edgelist\": \"../edgelists/wikipedia.edgelist\"},\n",
    "#                   \"protein-protein\": {\"edgelist\": \"../edgelists/ppi.edgelist\"},\n",
    "#                   \"ca-HepTh\": {\"edgelist\": \"../edgelists/ca-HepTh.edgelist\"},\n",
    "#                   \"AutonomousSystems\": {\"edgelist\": \"../edgelists/AS.edgelist\"},\n",
    "#                  }\n",
    "graph_metadata = {\"Facebook\": {\"edgelist\": \"../edgelists/facebook_combined.edgelist\", \n",
    "                                \"features\":\"../edgelists/facebook/node_genders.txt\"},\n",
    "                    # \"Ex1\": {\"edgelist\": \"../edgelists/facebook_combined.edgelist\", \n",
    "                    #             \"features\":\"../edgelists/facebook/node_genders.txt\"}          \n",
    "                 }\n",
    "embedding_algs = [\"Node2Vec\", \"HOPE\", \"HGCN\", \"LaplacianEigenmap\", \"SDNE\", \"SVD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate csv file with ... and InFoRM fairness score\n",
    "# for graph_name in graph_metadata:\n",
    "#     print(\"\\n\\n\" + graph_name)\n",
    "#     node_features = {}\n",
    "\n",
    "#     edgelist_file = graph_metadata[graph_name][\"edgelist\"]\n",
    "#     G = nx.read_edgelist(edgelist_file)\n",
    "#     get_node_pos(G, node_features)\n",
    "    \n",
    "#     for embedding_alg in embedding_algs:\n",
    "#         print(\"\\n\" + embedding_alg)\n",
    "#         embedding_file = \"../embeddings/{}/{}/{}_{}_64_embedding.npy\".format(graph_name, \n",
    "#                                                                              embedding_alg, \n",
    "#                                                                              graph_name, \n",
    "#                                                                              embedding_alg)\n",
    "#         embeddings = np.load(embedding_file)\n",
    "        \n",
    "#         node_features_copy = node_features.copy()\n",
    "#         get_inFoRM(G, embeddings, node_features_copy)\n",
    "#         get_pca_proj(G, embeddings, node_features_copy)\n",
    "        \n",
    "#         output_file = \"../embeddings/{}/{}/{}_{}_64_embedding_node_features.csv\".format(graph_name, \n",
    "#                                                                                          embedding_alg, \n",
    "#                                                                                          graph_name, \n",
    "#                                                                                          embedding_alg)\n",
    "#         with open(output_file, \"w\") as outputCSV:\n",
    "#             outputCSV.write(\"id, pos_x, pos_y, proj_x, proj_y, InFoRM\\n\")\n",
    "#             for node_id in node_features:\n",
    "#                 outputCSV.write(\"{}, {}, {}, {}, {}, {}\\n\".format(node_features[node_id][\"id\"],\n",
    "#                                                                   node_features[node_id][\"pos_x\"],\n",
    "#                                                                   node_features[node_id][\"pos_y\"],\n",
    "#                                                                   node_features[node_id][\"proj_x\"],\n",
    "#                                                                   node_features[node_id][\"proj_y\"],\n",
    "#                                                                   node_features[node_id][\"InFoRM\"]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Facebook\n",
      "\n",
      "Node2Vec\n",
      "\n",
      "HOPE\n",
      "\n",
      "HGCN\n",
      "\n",
      "LaplacianEigenmap\n",
      "\n",
      "SDNE\n",
      "\n",
      "SVD\n"
     ]
    }
   ],
   "source": [
    "# generate csv file with group fairness parameters and score\n",
    "# nodeid, sensitive attribute (S), value (z), k, g.f. score\n",
    "for graph_name in graph_metadata:\n",
    "    print(\"\\n\\n\" + graph_name)\n",
    "    # node_features = {}\n",
    "\n",
    "    edgelist_file = graph_metadata[graph_name][\"edgelist\"]\n",
    "    G = nx.read_edgelist(edgelist_file)\n",
    "    W = nx.to_numpy_array(G)\n",
    "    # get_node_pos(G, node_features)\n",
    "    \n",
    "    for embedding_alg in embedding_algs:\n",
    "        print(\"\\n\" + embedding_alg)\n",
    "        embedding_file = \"../embeddings/{}/{}/{}_{}_64_embedding.npy\".format(graph_name, \n",
    "                                                                             embedding_alg, \n",
    "                                                                             graph_name, \n",
    "                                                                             embedding_alg)\n",
    "        embedding = np.load(embedding_file)\n",
    "        node_features_file = graph_metadata[graph_name][\"features\"]\n",
    "        \n",
    "        # get sensitive attributes\n",
    "        with open(node_features_file, newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                sensitive_attrs = row[1:]\n",
    "                # print('sensitive_attrs',sensitive_attrs)\n",
    "                break\n",
    "\n",
    "        node_features = np.loadtxt(open(node_features_file, \"rb\"), delimiter=\",\", skiprows=1).astype(int)\n",
    "\n",
    "        # select output file\n",
    "        output_file_gf = \"../embeddings/{}/{}/{}_{}_64_embedding_group_fairness_scores_larger_ks.csv\".format(graph_name, \n",
    "                                                                                         embedding_alg, \n",
    "                                                                                         graph_name, \n",
    "                                                                                         embedding_alg)\n",
    "\n",
    "        # hay 4038 nodos en el embedding y 4035 tienen genero\n",
    "        # group_fairness_scores = group_unfairness_scores(Y, W, node_features, S, z, k)\n",
    "        # cambio el orden del for para poder calcular todas las scores\n",
    "        # si resolvemos los nodos faltantes se puede reajustar\n",
    "\n",
    "        # nodes = list(G.nodes())\n",
    "        # for i in range(len(inFoRM_scores)):\n",
    "        #     node_features[nodes[i]][\"InFoRM\"] = inFoRM_scores[i]\n",
    "\n",
    "        # create a dict nodeId->sensitive_attrs_val\n",
    "\n",
    "        with open(output_file_gf, \"w\") as outputCSV:\n",
    "            outputCSV.write(\"id,attribute,value,k,group_fairness_score\\n\")\n",
    "\n",
    "            for node_id in range(len(W)): # node ids start from 0\n",
    "                for attribute in sensitive_attrs:\n",
    "                    dict_node_attr_val = dict([(node_features[i,0],node_features[i,1]) for i in range(len(node_features))])\n",
    "                    # get values from the selected attribute\n",
    "                    attr_pos = sensitive_attrs.index(attribute) + 1\n",
    "                    sensitive_attrs_vals = np.unique(node_features[:,attr_pos])\n",
    "                    for value in sensitive_attrs_vals:\n",
    "                        for k in [5,10,20,25]: # define possible values for k\n",
    "                            # compute group fairness score\n",
    "                            if node_id in dict_node_attr_val.keys()  :\n",
    "                                score = group_unfairness_score(embedding, W, node_id, node_features, attr_pos, value, k)\n",
    "                            else:\n",
    "                                score = \"\"\n",
    "                            \n",
    "                                \n",
    "                            # score = group_unfairness_score(embedding, W, node_id, node_features, attr_pos, value, k)\n",
    "                            # write data to csv file\n",
    "                            outputCSV.write(\"{},{},{},{},{}\\n\".format(node_id,\n",
    "                                                    attribute,\n",
    "                                                    value,\n",
    "                                                    k,\n",
    "                                                    score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
